{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec.load('wiki_w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=pd.read_csv('new_weibo_data_clean.csv',names=['idx','content'])\n",
    "neg=pd.read_csv('new_weibo_data_clean0.csv',names=['idx','content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_arr=[]\n",
    "pos_la=[]\n",
    "for line in pos.values:\n",
    "    length=0\n",
    "    vecs=np.zeros((1,300))\n",
    "    newline=line[0].split(' ')\n",
    "    for i in newline:\n",
    "        try:\n",
    "            vecs+=model[i].reshape((1,300))\n",
    "            length+=1\n",
    "        except KeyError:\n",
    "            continue  \n",
    "    pos_arr.append(vecs/length)\n",
    "    pos_la.append(float(1))\n",
    "pos_arr=np.array(pos_arr).reshape(-1,300)\n",
    "pos_la=np.array(pos_la).reshape((-1,1))\n",
    "\n",
    "neg_arr=[]\n",
    "neg_la=[]\n",
    "for line in neg.values:\n",
    "    length=0\n",
    "    vecs=np.zeros((1,300))\n",
    "    newline=line[0].split(' ')\n",
    "    for i in newline:\n",
    "        try:\n",
    "            vecs+=model[i].reshape((1,300))\n",
    "            length+=1\n",
    "        except KeyError:\n",
    "            continue  \n",
    "    neg_arr.append(vecs/length)\n",
    "    neg_la.append(float(0))\n",
    "neg_arr=np.array(neg_arr).reshape(-1,300)\n",
    "neg_la=np.array(neg_la).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos=np.concatenate((pos_arr,pos_la),axis=1)\n",
    "data_neg=np.concatenate((neg_arr,neg_la),axis=1)\n",
    "data_pos=data_pos[~np.isnan(data_pos).any(axis=1)]\n",
    "data_neg=data_neg[~np.isnan(data_neg).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX=np.concatenate((data_pos[:,:-1],data_neg[:,:-1]),axis=0)\n",
    "dataY=np.concatenate((data_pos[:,-1],data_neg[:,-1]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x,test_x,train_y,test_y=train_test_split(dataX,dataY,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_models=[]\n",
    "for s in ['liblinear','lbfgs','newton-cg','sag']:\n",
    "    for m in [1000,3000,5000]:    #这三种迭代次数的结果是一样的\n",
    "        lr=LogisticRegression(max_iter=m,solver=s) \n",
    "        lr.fit(train_x,train_y)\n",
    "        lr_model_name=s+'_'+str(m)+'_'+'logistic'+'.pickle'\n",
    "        lr_models.append(lr_model_name)\n",
    "        with open(lr_model_name, 'wb') as fw:\n",
    "            pickle.dump(lr, fw)\n",
    "\n",
    "for m in lr_models:\n",
    "    with open(m, 'rb') as fr:\n",
    "        new_lr = pickle.load(fr)\n",
    "        print (m+str(new_lr.score(test_x,test_y)))\n",
    "\n",
    "#liblinear_1000_logistic.pickle0.8615847542627884  #与svc相比速度快很多\n",
    "# liblinear_1000_logistic.pickle0.8615847542627884\n",
    "# liblinear_3000_logistic.pickle0.8615847542627884\n",
    "# liblinear_5000_logistic.pickle0.8615847542627884\n",
    "# lbfgs_1000_logistic.pickle0.8615011701771983\n",
    "# lbfgs_3000_logistic.pickle0.8615011701771983\n",
    "# lbfgs_5000_logistic.pickle0.8615011701771983\n",
    "# newton-cg_1000_logistic.pickle0.8615011701771983\n",
    "# newton-cg_3000_logistic.pickle0.8615011701771983\n",
    "# newton-cg_5000_logistic.pickle0.8615011701771983\n",
    "# sag_1000_logistic.pickle0.8615011701771983\n",
    "# sag_3000_logistic.pickle0.8615011701771983\n",
    "# sag_5000_logistic.pickle0.8615011701771983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import neural_network\n",
    "nn_models=[]\n",
    "for s in ['lbfgs']:\n",
    "    for m in [3000]:    #这三种迭代次数的结果是一样的\n",
    "        for hls in [(300,50),(200,100)]:\n",
    "            nn=neural_network.MLPClassifier(hidden_layer_sizes=hls,solver=s,max_iter=m,early_stopping=True)\n",
    "            nn.fit(train_x,train_y)\n",
    "            nn_model_name=s+'_'+str(m)+'_'+str(hls)+'network'+'.pickle'\n",
    "            nn_models.append(nn_model_name)\n",
    "            with open(nn_model_name, 'wb') as fw:\n",
    "                pickle.dump(nn, fw)\n",
    "#lbfgs_3000_(200, 50)network.pickle0.9762621196924106\n",
    "\n",
    "nn_models=['adam_3000_(100, 50)network.pickle','adam_3000_(100,)network.pickle',\n",
    "'adam_3000_(200, 50)network.pickle',\n",
    "'lbfgs_3000_(100,)network.pickle',\n",
    "'lbfgs_3000_(200, 50)network.pickle',\n",
    "'lbfgs_3000_(300, 50)network.pickle',\n",
    "'lbfgs_3000_(200, 100)network.pickle',\n",
    "'sgd_3000_(100,)network.pickle',\n",
    "'sgd_3000_(200, 50)network.pickle']\n",
    "for m in nn_models:\n",
    "    with open(m, 'rb') as fr:\n",
    "        new_nn = pickle.load(fr)\n",
    "        print (m+str(new_nn.score(test_x,test_y)))\n",
    "\n",
    "# adam_3000_(100, 50)network.pickle0.9164994984954865\n",
    "# adam_3000_(100,)network.pickle0.9127382146439318\n",
    "# adam_3000_(200, 50)network.pickle0.9132397191574724\n",
    "# lbfgs_3000_(100,)network.pickle0.9758441992644601\n",
    "# lbfgs_3000_(200, 50)network.pickle0.9784353059177533\n",
    "# lbfgs_3000_(300, 50)network.pickle0.8593279839518556\n",
    "# lbfgs_3000_(200, 100)network.pickle0.8567368772985624\n",
    "# sgd_3000_(100,)network.pickle0.8849047141424273\n",
    "# sgd_3000_(200, 50)network.pickle0.9035439652290204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=64)  \n",
    "pca_fitted=pca.fit(train_x)\n",
    "with open('pca_demo64.pickle','wb') as fw:\n",
    "    pickle.dump(pca_fitted, fw)\n",
    "train_x=pca_fitted.transform(train_x)\n",
    "# print(pca.explained_variance_ratio_)\n",
    "test_x=pca_fitted.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "#设计不同参数的分类器并保存模型\n",
    "models=[]\n",
    "for k in ['linear','poly','rbf','sigmoid']:\n",
    "    for c in [0.1,1,2,8]:\n",
    "        clf = SVC(kernel=k,C=c)\n",
    "        clf.fit(train_x,train_y)  \n",
    "        model_name=k+'_'+str(c)+'_'+'svm'+'.pickle'\n",
    "        models.append(model_name)\n",
    "        with open(model_name, 'wb') as fw:\n",
    "            pickle.dump(clf, fw)\n",
    "\n",
    "\n",
    "for m in models:\n",
    "    with open(m, 'rb') as fr:\n",
    "        new_svm = pickle.load(fr)\n",
    "        print (m+str(new_svm.score(test_x,test_y)))\n",
    "#rbf_2_svm.pickle0.8753761283851554   #速度贼慢...\n",
    "# linear_0.1_svm.pickle0.8525576730190572\n",
    "# linear_1_svm.pickle0.8524740889334671\n",
    "# linear_2_svm.pickle0.8526412571046472\n",
    "# linear_8_svm.pickle0.8525576730190572\n",
    "# poly_0.1_svm.pickle0.8445336008024072\n",
    "# poly_1_svm.pickle0.8585757271815446\n",
    "# poly_2_svm.pickle0.859662320294216\n",
    "# poly_8_svm.pickle0.861250417920428\n",
    "# rbf_0.1_svm.pickle0.8658475426278837\n",
    "# rbf_1_svm.pickle0.8750417920427951\n",
    "# rbf_2_svm.pickle0.8753761283851554\n",
    "# rbf_8_svm.pickle0.8742895352724841\n",
    "# sigmoid_0.1_svm.pickle0.7433968572383818\n",
    "# sigmoid_1_svm.pickle0.7610330992978936\n",
    "# sigmoid_2_svm.pickle0.7608659311267135\n",
    "# sigmoid_8_svm.pickle0.7607823470411234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vec(sentence):\n",
    "    import jieba\n",
    "    # from gensim.models.word2vec import Word2Vec\n",
    "    # model = Word2Vec.load('wiki_w2v.model')\n",
    "    senvec=np.zeros((1,300))\n",
    "    newsen=jieba.cut(sentence)\n",
    "    length=0\n",
    "    for i in newsen:\n",
    "        try:\n",
    "            senvec+=model[i].reshape((1,300))\n",
    "            length+=1\n",
    "        except KeyError:\n",
    "            continue \n",
    "    return senvec/length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_label_split(sourse):\n",
    "    label=[]\n",
    "    data=[]    \n",
    "    for line in sourse.values:\n",
    "        label.append(float(line[0]))\n",
    "        data.append(sentence_vec(line[1]))\n",
    "    return np.array(data).reshape((-1,300)),np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas,labels=data_label_split(news)\n",
    "datas=datas[~np.isnan(datas).any(axis=1)]\n",
    "labels=np.ones(datas.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad=pd.read_csv('bad.csv',names=['price','describe'],sep='\\t')\n",
    "bad=bad.drop(index=0)\n",
    "bad_datas,bad_labels=data_label_split(bad)\n",
    "bad_datas=bad_datas[~np.isnan(bad_datas).any(axis=1)]\n",
    "bad_labels=np.zeros(bad_datas.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad2=pd.read_csv('bad2.csv',names=['price','describe'],sep='\\t')\n",
    "bad2=bad2.drop(index=0)\n",
    "bad2_datas,bad2_labels=data_label_split(bad2)\n",
    "bad2_datas=bad2_datas[~np.isnan(bad2_datas).any(axis=1)]\n",
    "bad2_labels=np.zeros(bad2_datas.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badX=np.concatenate((bad2_datas,bad_datas),axis=0)\n",
    "badY=np.concatenate((bad2_labels,bad_labels),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtestX=np.concatenate((datas,badX),axis=0)\n",
    "newtestY=np.concatenate((labels,badY),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "for models in [\n",
    "    'adam_3000_(100, 50)network.pickle',\n",
    "    'adam_3000_(100,)network.pickle',\n",
    "    'adam_3000_(200, 50)network.pickle',\n",
    "    'lbfgs_3000_(100,)network.pickle',\n",
    "    'lbfgs_3000_(200, 50)network.pickle',\n",
    "    'lbfgs_3000_(200, 100)network.pickle',\n",
    "    'lbfgs_3000_(300, 50)network.pickle',\n",
    "    'sgd_3000_(200, 50)network.pickle',\n",
    "    'sgd_3000_(100,)network.pickle',\n",
    "    'lbfgs_1000_logistic.pickle',\n",
    "    'liblinear_1000_logistic.pickle',\n",
    "    'newton-cg_3000_logistic.pickle',\n",
    "    'sag_1000_logistic.pickle'\n",
    "#     'test_rbf_2_svm.pickle'  #需要降维\n",
    "]:\n",
    "    with open(models, 'rb') as fr:\n",
    "        new_nn = pickle.load(fr)\n",
    "        print(models +str(new_nn.score(datas,labels)))\n",
    "\n",
    "#模型在好评上的表现\n",
    "# adam_3000_(100, 50)network.pickle0.8759842519685039\n",
    "# adam_3000_(100,)network.pickle0.8523622047244095\n",
    "# adam_3000_(200, 50)network.pickle0.8228346456692913\n",
    "# lbfgs_3000_(100,)network.pickle0.7598425196850394\n",
    "# lbfgs_3000_(200, 50)network.pickle0.7716535433070866\n",
    "# lbfgs_3000_(200, 100)network.pickle0.7736220472440944\n",
    "# lbfgs_3000_(300, 50)network.pickle0.7460629921259843\n",
    "# sgd_3000_(200, 50)network.pickle0.8405511811023622\n",
    "# sgd_3000_(100,)network.pickle0.7992125984251969\n",
    "# lbfgs_1000_logistic.pickle0.7795275590551181\n",
    "# liblinear_1000_logistic.pickle0.7775590551181102\n",
    "# newton-cg_3000_logistic.pickle0.7775590551181102\n",
    "# sag_1000_logistic.pickle0.7775590551181102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "for models in [\n",
    "    'adam_3000_(100, 50)network.pickle',\n",
    "    'adam_3000_(100,)network.pickle',\n",
    "    'adam_3000_(200, 50)network.pickle',\n",
    "    'lbfgs_3000_(100,)network.pickle',\n",
    "    'lbfgs_3000_(200, 50)network.pickle',\n",
    "    'lbfgs_3000_(200, 100)network.pickle',\n",
    "    'lbfgs_3000_(300, 50)network.pickle',\n",
    "    'sgd_3000_(200, 50)network.pickle',\n",
    "    'sgd_3000_(100,)network.pickle',\n",
    "    'lbfgs_1000_logistic.pickle',\n",
    "    'liblinear_1000_logistic.pickle',\n",
    "    'newton-cg_3000_logistic.pickle',\n",
    "    'sag_1000_logistic.pickle'\n",
    "#     'test_rbf_2_svm.pickle'  #需要降维\n",
    "]:\n",
    "    with open(models, 'rb') as fr:\n",
    "        new_nn = pickle.load(fr)\n",
    "        print(models +str(new_nn.score(badX,badY)))\n",
    "        \n",
    "#模型在差评上的表现\n",
    "# adam_3000_(100, 50)network.pickle0.6831325301204819\n",
    "# adam_3000_(100,)network.pickle0.744578313253012\n",
    "# adam_3000_(200, 50)network.pickle0.7518072289156627\n",
    "# lbfgs_3000_(100,)network.pickle0.6771084337349398\n",
    "# lbfgs_3000_(200, 50)network.pickle0.736144578313253\n",
    "# lbfgs_3000_(200, 100)network.pickle0.7240963855421687\n",
    "# lbfgs_3000_(300, 50)network.pickle0.7108433734939759\n",
    "# sgd_3000_(200, 50)network.pickle0.7048192771084337\n",
    "# sgd_3000_(100,)network.pickle0.7240963855421687\n",
    "# lbfgs_1000_logistic.pickle0.7518072289156627\n",
    "# liblinear_1000_logistic.pickle0.7518072289156627\n",
    "# newton-cg_3000_logistic.pickle0.7518072289156627\n",
    "# sag_1000_logistic.pickle0.7518072289156627"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pca_demo64.pickle','rb') as fr:\n",
    "    pca_demo=pickle.load(fr)\n",
    "    data_pcaed=pca_demo.transform(newtestX)\n",
    "    print(data_pcaed.shape)\n",
    "for m in [\n",
    "'linear_0.1_svm.pickle',\n",
    "'linear_1_svm.pickle',\n",
    "'linear_2_svm.pickle',\n",
    "'linear_8_svm.pickle',\n",
    "'poly_0.1_svm.pickle',\n",
    "'poly_1_svm.pickle',\n",
    "'poly_2_svm.pickle',\n",
    "'poly_8_svm.pickle',\n",
    "'rbf_0.1_svm.pickle',\n",
    "'rbf_1_svm.pickle',\n",
    "'rbf_2_svm.pickle',\n",
    "'rbf_8_svm.pickle',\n",
    "'sigmoid_0.1_svm.pickle',\n",
    "'sigmoid_1_svm.pickle',\n",
    "'sigmoid_2_svm.pickle',\n",
    "'sigmoid_8_svm.pickle',\n",
    "]:\n",
    "    with open(m, 'rb') as fr:\n",
    "        new_model= pickle.load(fr)\n",
    "        print(m +str(new_model.score(data_pcaed,newtestY)))\n",
    "\n",
    "#差评多数、好评少数时的表现：\n",
    "#poly_0.1_svm.pickle0.7675635276532138\n",
    "# rbf_0.1_svm.pickle0.7630792227204783    \n",
    "# rbf_2_svm.pickle0.7615844544095666\n",
    "\n",
    "# linear_0.1_svm.pickle0.7511210762331838\n",
    "# linear_1_svm.pickle0.7511210762331838\n",
    "# linear_2_svm.pickle0.7511210762331838\n",
    "# linear_8_svm.pickle0.7511210762331838\n",
    "# poly_0.1_svm.pickle0.7675635276532138\n",
    "# poly_1_svm.pickle0.750373692077728\n",
    "# poly_2_svm.pickle0.7466367713004485\n",
    "# poly_8_svm.pickle0.7414050822122571\n",
    "# rbf_0.1_svm.pickle0.7630792227204783\n",
    "# rbf_1_svm.pickle0.7593423019431988\n",
    "# rbf_2_svm.pickle0.7615844544095666\n",
    "# rbf_8_svm.pickle0.7316890881913304\n",
    "# sigmoid_0.1_svm.pickle0.6741405082212257\n",
    "# sigmoid_1_svm.pickle0.6935724962630793\n",
    "# sigmoid_2_svm.pickle0.6928251121076233\n",
    "# sigmoid_8_svm.pickle0.6935724962630793\n",
    "\n",
    "#好评结果   # rbf_0.1_svm.pickle0.8385826771653543\n",
    "# linear_0.1_svm.pickle0.8011811023622047\n",
    "# linear_1_svm.pickle0.8011811023622047\n",
    "# linear_2_svm.pickle0.8011811023622047\n",
    "# linear_8_svm.pickle0.8011811023622047\n",
    "# poly_0.1_svm.pickle0.7480314960629921\n",
    "# poly_1_svm.pickle0.7539370078740157\n",
    "# poly_2_svm.pickle0.75\n",
    "# poly_8_svm.pickle0.7696850393700787\n",
    "# rbf_0.1_svm.pickle0.8385826771653543\n",
    "# rbf_1_svm.pickle0.8208661417322834\n",
    "# rbf_2_svm.pickle0.8149606299212598\n",
    "# rbf_8_svm.pickle0.7854330708661418\n",
    "# sigmoid_0.1_svm.pickle0.5728346456692913\n",
    "# sigmoid_1_svm.pickle0.6633858267716536\n",
    "# sigmoid_2_svm.pickle0.6633858267716536\n",
    "# sigmoid_8_svm.pickle0.6633858267716536\n",
    "\n",
    "\n",
    "#差评上   # poly_0.1_svm.pickle0.8200589970501475  # poly_0.1_svm.pickle0.7795180722891566\n",
    "# linear_0.1_svm.pickle0.7204819277108434\n",
    "# linear_1_svm.pickle0.7204819277108434\n",
    "# linear_2_svm.pickle0.7204819277108434\n",
    "# linear_8_svm.pickle0.7204819277108434\n",
    "# poly_0.1_svm.pickle0.7795180722891566\n",
    "# poly_1_svm.pickle0.7481927710843373\n",
    "# poly_2_svm.pickle0.744578313253012\n",
    "# poly_8_svm.pickle0.7240963855421687\n",
    "# rbf_0.1_svm.pickle0.7168674698795181\n",
    "# rbf_1_svm.pickle0.7216867469879518\n",
    "# rbf_2_svm.pickle0.7289156626506024\n",
    "# rbf_8_svm.pickle0.6987951807228916\n",
    "# sigmoid_0.1_svm.pickle0.736144578313253\n",
    "# sigmoid_1_svm.pickle0.7120481927710843\n",
    "# sigmoid_2_svm.pickle0.7108433734939759\n",
    "# sigmoid_8_svm.pickle0.7120481927710843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型在新的好评与差评上的表现（差评居多）\n",
    "# adam_3000_(100, 50)network.pickle0.7563527653213752\n",
    "# adam_3000_(100,)network.pickle0.7855007473841554\n",
    "# adam_3000_(200, 50)network.pickle0.7787742899850523\n",
    "#poly_0.1_svm.pickle0.7675635276532138\n",
    "# rbf_0.1_svm.pickle0.7630792227204783    \n",
    "# rbf_2_svm.pickle0.7615844544095666\n",
    "# （lbfgs_1000_logistic.pickle0.7623318385650224\n",
    "# liblinear_1000_logistic.pickle0.7615844544095666\n",
    "# newton-cg_3000_logistic.pickle0.7615844544095666\n",
    "# sag_1000_logistic.pickle0.7615844544095666）\n",
    "\n",
    "#差评表现：\n",
    "# poly_0.1_svm.pickle0.7795180722891566\n",
    "# adam_3000_(100,)network.pickle0.744578313253012\n",
    "# adam_3000_(200, 50)network.pickle0.7518072289156627\n",
    "# （lbfgs_1000_logistic.pickle0.7518072289156627\n",
    "# liblinear_1000_logistic.pickle0.7518072289156627\n",
    "# newton-cg_3000_logistic.pickle0.7518072289156627\n",
    "# sag_1000_logistic.pickle0.7518072289156627）\n",
    "\n",
    "#好评表现：\n",
    "# adam_3000_(100, 50)network.pickle0.8759842519685039\n",
    "# adam_3000_(100,)network.pickle0.8523622047244095\n",
    "# adam_3000_(200, 50)network.pickle0.8228346456692913\n",
    "# rbf_0.1_svm.pickle0.8385826771653543\n",
    "# rbf_1_svm.pickle0.8208661417322834\n",
    "# （lbfgs_1000_logistic.pickle0.7795275590551181\n",
    "# liblinear_1000_logistic.pickle0.7775590551181102\n",
    "# newton-cg_3000_logistic.pickle0.7775590551181102\n",
    "# sag_1000_logistic.pickle0.7775590551181102）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
