{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=pd.read_csv('new_weibo_data_clean.csv',names=['idx','content'])\n",
    "neg=pd.read_csv('new_weibo_data_clean0.csv',names=['idx','content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-4ab726e1ddac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# count=0\n",
    "# # train_x=train_x[~np.isnan(train_x).any(axis=1)]\n",
    "# for line in pos.values:\n",
    "#     count+=1\n",
    "#     if np.isnan(line):\n",
    "#         print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec.load('wiki_w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "f:\\python\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# output_file_name = 'weibo1.txt'\n",
    "# output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "# label=1\n",
    "# for line in pos.values:\n",
    "#     length=0\n",
    "#     vecs=np.zeros((1,300))\n",
    "#     newline=line[0].split(' ')\n",
    "#     for i in newline:\n",
    "#         try:\n",
    "#             vecs+=model[i].reshape((1,300))\n",
    "#             length+=1\n",
    "#         except KeyError:\n",
    "#             continue  \n",
    "\n",
    "#     otp = \"{0} {1}\\n\".format((vecs/length),label)\n",
    "#     output_file.write(otp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n",
      "f:\\python\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "pos_arr=[]\n",
    "pos_la=[]\n",
    "for line in pos.values:\n",
    "    length=0\n",
    "    vecs=np.zeros((1,300))\n",
    "    newline=line[0].split(' ')\n",
    "    for i in newline:\n",
    "        try:\n",
    "            vecs+=model[i].reshape((1,300))\n",
    "            length+=1\n",
    "        except KeyError:\n",
    "            continue  \n",
    "    pos_arr.append(vecs/length)\n",
    "    pos_la.append(float(1))\n",
    "pos_arr=np.array(pos_arr).reshape(-1,300)\n",
    "pos_la=np.array(pos_la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n",
      "f:\\python\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "neg_arr=[]\n",
    "neg_la=[]\n",
    "for line in neg.values:\n",
    "    length=0\n",
    "    vecs=np.zeros((1,300))\n",
    "    newline=line[0].split(' ')\n",
    "    for i in newline:\n",
    "        try:\n",
    "            vecs+=model[i].reshape((1,300))\n",
    "            length+=1\n",
    "        except KeyError:\n",
    "            continue  \n",
    "    neg_arr.append(vecs/length)\n",
    "    neg_la.append(float(0))\n",
    "neg_arr=np.array(neg_arr).reshape(-1,300)\n",
    "neg_la=np.array(neg_la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_la=neg_la.reshape((-1,1))\n",
    "pos_la=pos_la.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合并与空值去除\n",
    "data_pos=np.concatenate((pos_arr,pos_la),axis=1)\n",
    "data_neg=np.concatenate((neg_arr,neg_la),axis=1)\n",
    "data_pos=data_pos[~np.isnan(data_pos).any(axis=1)]\n",
    "data_neg=data_neg[~np.isnan(data_neg).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX=np.concatenate((data_pos[:,:-1],data_neg[:,:-1]),axis=0)\n",
    "dataY=np.concatenate((data_pos[:,-1],data_neg[:,-1]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x,test_x,train_y,test_y=train_test_split(dataX,dataY,test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107676, 300)\n",
      "(11964, 300)\n",
      "(107676,)\n",
      "(11964,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8552323637579405"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import neural_network\n",
    "clf=neural_network.MLPClassifier()\n",
    "clf.fit(train_x,train_y)\n",
    "clf.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=64)  \n",
    "pca_fitted=pca.fit(train_x)\n",
    "train_x=pca_fitted.transform(train_x)\n",
    "test_x=pca_fitted.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "#设计不同参数的分类器并保存模型\n",
    "models=[]\n",
    "for k in ['linear','poly','rbf','sigmoid']:\n",
    "    for c in [0.1,1,2,8]:\n",
    "        clf = SVC(kernel=k,C=c)\n",
    "        clf.fit(train_x,train_y)\n",
    "        model_name=k+'_'+str(c)+'_'+'svm'+'.pickle'\n",
    "        models.append(model_name)\n",
    "        with open(model_name, 'wb') as fw:\n",
    "            pickle.dump(clf, fw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models:\n",
    "    with open(m, 'rb') as fr:\n",
    "        new_svm = pickle.load(fr)\n",
    "        print (m+str(new_svm.score(test_x,test_y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#进步进行降维都可以\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_models=[]\n",
    "for s in ['liblinear','lbfgs','newton-cg','sag']:\n",
    "    for m in [1000,3000,5000]:    #这三种迭代次数的结果是一样的\n",
    "        lr=LogisticRegression(max_iter=m,solver=s) \n",
    "        lr.fit(train_x,train_y)\n",
    "        lr_model_name=s+'_'+str(m)+'_'+'logistic'+'.pickle'\n",
    "        lr_models.append(lr_model_name)\n",
    "        with open(lr_model_name, 'wb') as fw:\n",
    "            pickle.dump(lr, fw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in lr_models:\n",
    "    with open(m, 'rb') as fr:\n",
    "        new_lr = pickle.load(fr)\n",
    "        print (m+str(new_lr.score(test_x,test_y)))\n",
    "\n",
    "#liblinear_1000_logistic.pickle0.8615847542627884  #与svc相比速度快很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liblinear_1000_logistic.pickle0.8615847542627884\n",
    "# liblinear_3000_logistic.pickle0.8615847542627884\n",
    "# liblinear_5000_logistic.pickle0.8615847542627884\n",
    "# lbfgs_1000_logistic.pickle0.8615011701771983\n",
    "# lbfgs_3000_logistic.pickle0.8615011701771983\n",
    "# lbfgs_5000_logistic.pickle0.8615011701771983\n",
    "# newton-cg_1000_logistic.pickle0.8615011701771983\n",
    "# newton-cg_3000_logistic.pickle0.8615011701771983\n",
    "# newton-cg_5000_logistic.pickle0.8615011701771983\n",
    "# sag_1000_logistic.pickle0.8615011701771983\n",
    "# sag_3000_logistic.pickle0.8615011701771983\n",
    "# sag_5000_logistic.pickle0.8615011701771983"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
